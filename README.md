# LMFFT

## Introduction
This repository contains the code and data for the paper titled "Language model encoded multi-scale feature fusion and transformation for predicting protein-peptide binding sites".  The paper introduces a novel sequence-based end-to-end PPBS predictor using deep learning, named Language model encoded Multi-scale Feature Fusion and Transformation (LMFFT). The proposed model starts with a single protein language model for comprehensive multi-scale feature extraction, including residue, dipeptide, and fragment-level representations, which are implemented by the dipeptide embedding-based fragment fusion and further enhanced through the dipeptide contextual encoding. Moreover, multi-scale convolutional neural networks are applied to transform multi-scale features by capturing intricate interactions between local and global information. Our LMFFT achieves state-of-the-art performance across three benchmark datasets, outperforming existing sequence-based methods and demonstrating competitive advantages over certain structure-based baselines. This work provides a cost-effective and efficient solution for PPBS prediction, advancing revealing the sequence-function relationship of proteins.

## ğŸ“ Project ğŸ“
```markdown
.gitignore           # Gitå¿½ç•¥æ–‡ä»¶é…ç½®
data.py              # æ•°æ®å¤„ç†ç›¸å…³è„šæœ¬
model.py             # æ¨¡å‹å®šä¹‰ä¸ç›¸å…³ä»£ç 
README.md            # é¡¹ç›®è¯´æ˜æ–‡æ¡£
run.py               # ä¸»è¿è¡Œè„šæœ¬
train_eval.py        # è®­ç»ƒä¸è¯„ä¼°è„šæœ¬
data/                # å­˜æ”¾æ•°æ®é›†çš„æ–‡ä»¶å¤¹
    Dataset1_test.tsv
    Dataset1_train.tsv
    Dataset2_test.tsv
    Dataset2_train.tsv
    Dataset3_test.tsv
    Dataset3_train.tsv
pkls/                # å­˜æ”¾æ¨¡å‹æƒé‡æ–‡ä»¶çš„æ–‡ä»¶å¤¹
util/                # å·¥å…·å‡½æ•°
    util_metric.py
```

## âš™ï¸ Setup  âš™ï¸
è¦åœ¨æœ¬ä»“åº“ä¸­è¿è¡Œä»£ç ï¼Œæ‚¨éœ€è¦ä»¥ä¸‹ä¾èµ–é¡¹ï¼š<br>To run the code in this repository, you'll need the following dependencies:
- python 3.10.14
- torch 2.2.2
- transformers 4.39.3


## ğŸ¤– Download  ğŸ¤–
åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¹‹å‰ï¼Œæ‚¨éœ€è¦ä¸‹è½½æ•°æ®é›†å¹¶å°†å…¶æ”¾ç½®åœ¨ ./data ç›®å½•ä¸­ã€‚<br>Before training and testing, you need to download the dataset and place it in the ./data directory.

åœ¨æ‰§è¡Œä»£ç ä¹‹å‰ï¼Œæ‚¨éœ€è¦ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹[esm2_t33_650M_UR50D-finetuned-secondary-structure](https://huggingface.co/gaodrew/esm2_t33_650M_UR50D-finetuned-secondary-structure)ï¼Œå¹¶å°†å®ƒä»¬æ”¾ç½®åœ¨ ./pretrained_model ç›®å½•ä¸­ã€‚<br>Before executing the code, you need to download the pre-trained model [esm2_t33_650M_UR50D-finetuned-secondary-structure](https://huggingface.co/gaodrew/esm2_t33_650M_UR50D-finetuned-secondary-structure) , pubmedbert and place them in the ./pretrained_model directory.

## âš¡ï¸ Running the Code  âš¡ï¸
- Model Training:
```python
python run.py --do_train --dataset <dataset_num: 1, 2, 3>
```

- Model Testing:
ä¸‹è½½[æ¨¡å‹](https://drive.google.com/drive/folders/1Vrf7G1rzmW5sezYpwwTAHnXbLHT2R5P3?usp=drive_link)å¹¶å°†å…¶æ”¾ç½®åœ¨ ./pkls ç›®å½•ä¸­ã€‚<br>Download the [model](https://drive.google.com/drive/folders/1Vrf7G1rzmW5sezYpwwTAHnXbLHT2R5P3?usp=drive_link) and place it in the ./pkls directory.
```python
python run.py --eval --dataset <dataset_num: 1, 2, 3>
```




